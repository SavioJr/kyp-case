"""
llm.py
Camada de IA Generativa (provider-agnostic, aqui usando Groq).

üìå O*NET Task 1:
"Analyze credit data and financial statements to determine the degree of risk involved in extending 
credit or lending money."

Aqui a IA:
- interpreta os n√∫meros (ratios + dados)
- produz um risco preliminar + justificativa
- gera perguntas/pontos de aten√ß√£o para revis√£o humana
"""

from typing import Dict, Any
from groq import Groq
from dotenv import load_dotenv
import os

load_dotenv()

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")  # default seguro


def build_prompt(context: Dict[str, Any]) -> str:
    return f"""
Voc√™ √© um analista de cr√©dito s√™nior auxiliando uma equipe humana.
Seu objetivo √© produzir uma AN√ÅLISE PRELIMINAR de risco, clara e estruturada.

‚ö†Ô∏è IMPORTANTE:
- N√£o invente dados.
- Use APENAS as informa√ß√µes fornecidas.
- Se algo estiver faltando, indique explicitamente.
- Seja direto, profissional e organizado.
- N√ÉO fa√ßa decis√£o final, apenas triagem.

Responda OBRIGATORIAMENTE no formato abaixo:

RISCO: <BAIXO | M√âDIO | ALTO>

RESUMO EXECUTIVO:
<2‚Äì3 frases objetivas resumindo o perfil de risco>

FUNDAMENTOS DO RISCO:
- <ligar indicadores financeiros a risco>
- <ligar liquidez ao prazo do receb√≠vel>
- <ligar endividamento √† sustentabilidade>

PONTOS DE ATEN√á√ÉO:
- <itens que N√ÉO impedem a opera√ß√£o, mas exigem cuidado>

PERGUNTAS PARA VALIDA√á√ÉO HUMANA:
- <quest√µes que um analista deveria verificar antes da decis√£o>

DADOS (JSON):
{context}
""".strip()


def call_llm(context: Dict[str, Any]) -> str:
    if not GROQ_API_KEY:
        raise RuntimeError("GROQ_API_KEY n√£o configurada no .env")

    client = Groq(api_key=GROQ_API_KEY)
    prompt = build_prompt(context)

    resp = client.chat.completions.create(
        model=GROQ_MODEL,
        messages=[
            {"role": "system", "content": "Responda em portugu√™s do Brasil, tom profissional e direto."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2, # respostas mais focadas e consistentes
        max_tokens=600, # controle de custo
    )

    return resp.choices[0].message.content.strip()